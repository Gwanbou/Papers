<h1 align="center">
Paper Vault
</h1>
<p align="center">
:bulb: This repository comprises a collection of research articles relevant to <i><b>natural language processing</i></b>, <i><b>deep tabular model</i></b>, <i><b>knowledge graph</i></b>, and <i><b>general AI topics</i></b>. Please feel free to create a PR if you come across any interesting papers. :raised_hands:
</p>

## Automation

1. Rename and put downloaded article into categorical folder.
2. Run `automation/automation.py`.
3. Add :white_check_mark: `:white_check_mark:` manually while you complete reading an article.

## :newspaper_roll: DOX

<!-- AUTOGENERATED_DOX -->
- [ ] [:link:](./files/DOX/[2018%20@sap%20Katti]%20Chargrid_Towards%20Understanding%202D%20Documents.pdf)
  Chargrid: Towards Understanding 2D Documents
  ![year](https://img.shields.io/badge/2018-green?style=flat)
  ![auth](https://img.shields.io/badge/Katti-red?style=flat)
  ![tags](https://img.shields.io/badge/sap-blue?style=flat)
- [ ] [:link:](./files/DOX/[2019%20@microsoft%20Xu]%20LayoutLM_Pre-training%20of%20Text%20and%20Layout%20for%20Document%20Image%20Understanding.pdf)
  LayoutLM: Pre-training of Text and Layout for Document Image Understanding
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Xu-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/DOX/[2020%20@sap%20Reisswig]%20Chargrid-OCR_End-to-end%20Trainable%20Optical%20Character%20Recognition%20for%20Printed%20Documents%20using%20Instance%20Segmentation.pdf)
  Chargrid-OCR: End-to-end Trainable Optical Character Recognition for Printed Documents using Instance Segmentation
  ![year](https://img.shields.io/badge/2020-green?style=flat)
  ![auth](https://img.shields.io/badge/Reisswig-red?style=flat)
  ![tags](https://img.shields.io/badge/sap-blue?style=flat)
- [ ] [:link:](./files/DOX/[2020%20Li]%20A%20Survey%20on%20Deep%20Learning%20for%20Named%20Entity%20Recognition.pdf)
  A Survey on Deep Learning for Named Entity Recognition
  ![year](https://img.shields.io/badge/2020-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
- [ ] [:link:](./files/DOX/[2021%20@microsoft%20Li]%20TrOCR_Transformer-based%20Optical%20Character%20Recognition%20with%20Pre-trained%20Models.pdf)
  TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/DOX/[2022%20@acl%20Wang]%20LiLT_A%20Simple%20yet%20Effective%20Language-Independent%20Layout%20Transformer%20for%20Structured%20Document%20Understanding.pdf)
  LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2022%20@ibm%20Pfitzmann]%20DocLayNet_A%20Large%20Human-Annotated%20Dataset%20for%20Document-Layout%20Analysis.pdf)
  DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Pfitzmann-red?style=flat)
  ![tags](https://img.shields.io/badge/ibm-blue?style=flat)
- [ ] [:link:](./files/DOX/[2023%20@acl%20Ren]%20Retrieve-and-Sample_Document-level%20Event%20Argument%20Extraction%20via%20Hybrid%20Retrieval%20Augmentation.pdf)
  Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Ren-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2023%20@cvpr%20@microsoft%20Tang]%20Unifying%20Vision,%20Text,%20and%20Layout%20for%20Universal%20Document%20Processing.pdf)
  Unifying Vision, Text, and Layout for Universal Document Processing
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Tang-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/DOX/[2023%20@emnlp%20Yu]%20DocumentNet_Bridging%20the%20Data%20Gap%20in%20Document%20Pre-Training.pdf)
  DocumentNet: Bridging the Data Gap in Document Pre-Training
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Yu-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/DOX/[2023%20@meta%20Blecher]%20Nougat_Neural%20Optical%20Understanding%20for%20Academic%20Documents.pdf)
  Nougat: Neural Optical Understanding for Academic Documents
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Blecher-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/DOX/[2023%20Han]%20ChartLlama_A%20Multimodal%20LLM%20for%20Chart%20Understanding%20and%20Generation.pdf)
  ChartLlama: A Multimodal LLM for Chart Understanding and Generation
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Han-red?style=flat)
- [ ] [:link:](./files/DOX/[2023%20Masry]%20UniChart_A%20Universal%20Vision-language%20Pretrained%20Model%20for%20Chart%20Comprehension%20and%20Reasoning.pdf)
  UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Masry-red?style=flat)
- [ ] [:link:](./files/DOX/[2023%20Ozyurt]%20Document-Level%20In-Context%20Few-Shot%20Relation%20Extraction%20via%20Pre-Trained%20Language%20Models.pdf)
  Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Ozyurt-red?style=flat)
- [ ] [:link:](./files/DOX/[2023%20Wang]%20Layout%20and%20Task%20Aware%20Instruction%20Prompt%20for%20Zero-shot%20Document%20Image%20Question%20Answering.pdf)
  Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
- [ ] [:link:](./files/DOX/[2023%20Wang]%20VRDU_A%20Benchmark%20for%20Visually-rich%20Document%20Understanding.pdf)
  VRDU: A Benchmark for Visually-rich Document Understanding
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@aaai%20Hu]%20DocMamba_Efficient%20Document%20Pre-training%20with%20State%20Space%20Model.pdf)
  DocMamba: Efficient Document Pre-training with State Space Model
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Hu-red?style=flat)
  ![tags](https://img.shields.io/badge/aaai-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@aaai%20Wang]%20Knowledge%20Graph%20Prompting%20for%20Multi-Document%20Question%20Answering.pdf)
  Knowledge Graph Prompting for Multi-Document Question Answering
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/aaai-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@aaai%20Zhang]%20DocKylin_A%20Large%20Multimodal%20Model%20for%20Visual%20Document%20Understanding%20with%20Efficient%20Visual%20Slimming.pdf)
  DocKylin: A Large Multimodal Model for Visual Document Understanding with Efficient Visual Slimming
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhang-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20@cohere%20Wang]%20DAPR_A%20Benchmark%20on%20Document-Aware%20Passage%20Retrieval.pdf)
  DAPR: A Benchmark on Document-Aware Passage Retrieval
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/cohere-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Deng]%20Document-level%20Claim%20Extraction%20and%20Decontextualisation%20for%20Fact-Checking.pdf)
  Document-level Claim Extraction and Decontextualisation for Fact-Checking
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Deng-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Gao]%20TTM-RE_Memory-Augmented%20Document-Level%20Relation%20Extraction.pdf)
  TTM-RE: Memory-Augmented Document-Level Relation Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Gao-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Li]%20Hypergraph%20based%20Understanding%20for%20Document%20Semantic%20Entity%20Recognition.pdf)
  Hypergraph based Understanding for Document Semantic Entity Recognition
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Min]%20Synergetic%20Event%20Understanding_A%20Collaborative%20Approach%20to%20Cross-Document%20Event%20Coreference%20Resolution%20with%20Large%20Language%20Models.pdf)
  Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Min-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Na]%20Reward-based%20Input%20Construction%20for%20Cross-document%20Relation%20Extraction.pdf)
  Reward-based Input Construction for Cross-document Relation Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Na-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Qi]%20End-to-end%20Learning%20of%20Logical%20Rules%20for%20Enhancing%20Document-level%20Relation%20Extraction.pdf)
  End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Qi-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Wang]%20DocLLM_A%20layout-aware%20generative%20language%20model%20for%20multimodal%20document%20understanding.pdf)
  DocLLM: A layout-aware generative language model for multimodal document understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Xhen]%20MetaSumPerceiver_Multimodal%20Multi-Document%20Evidence%20Summarization%20for%20Fact-Checking.pdf)
  MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Xhen-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Zhao]%20DocMath-Eval_Evaluating%20Math%20Reasoning%20Capabilities%20of%20LLMs%20in%20Understanding%20Financial%20Documents.pdf)
  DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhao-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Zhou]%20LLMs%20Learn%20Task%20Heuristics%20from%20Demonstrations_A%20Heuristic-Driven%20Prompting%20Strategy%20for%20Document-Level%20Event%20Argument%20Extraction.pdf)
  LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhou-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@acl%20Zhu]%20FanOutQA_A%20Multi-Hop,%20Multi-Document%20Question%20Answering%20Benchmark%20for%20Large%20Language%20Models.pdf)
  FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhu-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@alibaba%20Liao]%20DocLayLLM_An%20Efficient%20and%20Effective%20Multi-modal%20Extension%20of%20Large%20Language%20Models%20for%20Text-rich%20Document%20Understanding.pdf)
  DocLayLLM: An Efficient and Effective Multi-modal Extension of Large Language Models for Text-rich Document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Liao-red?style=flat)
  ![tags](https://img.shields.io/badge/alibaba-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@alibaba%20Luo]%20LayoutLLM_Layout%20Instruction%20Tuning%20with%20Large%20Language%20Models%20for%20Document%20Understanding.pdf)
  LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Luo-red?style=flat)
  ![tags](https://img.shields.io/badge/alibaba-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@alibaba%20Wan]%20OmniParser_A%20Unified%20Framework%20for%20Text%20Spotting,%20Key%20Information%20Extraction%20and%20Table%20Recognition.pdf)
  OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wan-red?style=flat)
  ![tags](https://img.shields.io/badge/alibaba-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@amazon%20@stanford%20Sinha]%20Guiding%20Vision-Language%20Model%20Selection%20for%20Visual%20Question-Answering%20Across%20Tasks,%20Domains,%20and%20Knowledge%20Types.pdf)
  Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Sinha-red?style=flat)
  ![tags](https://img.shields.io/badge/stanford-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@aws%20Liao]%20DocTr_Document%20Transformer%20for%20Structured%20Information%20Extraction%20in%20Documents.pdf)
  DocTr: Document Transformer for Structured Information Extraction in Documents
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Liao-red?style=flat)
  ![tags](https://img.shields.io/badge/aws-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@cornell%20Morris]%20Contextual%20Document%20Embeddings.pdf)
  Contextual Document Embeddings
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Morris-red?style=flat)
  ![tags](https://img.shields.io/badge/cornell-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20@adobe%20Yang]%20FIZZ_Factual%20Inconsistency%20Detection%20by%20Zoom-in%20Summary%20and%20Zoom-out%20Document.pdf)
  FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Yang-red?style=flat)
  ![tags](https://img.shields.io/badge/adobe-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20@alibaba%20Hu]%20mPLUG-DocOwl%201.5_Unified%20Structure%20Learning%20for%20OCR-free%20Document%20Understanding.pdf)
  mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Hu-red?style=flat)
  ![tags](https://img.shields.io/badge/alibaba-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20@alibaba%20Wang]%20Leave%20No%20Document%20Behind_Benchmarking%20Long-Context%20LLMs%20with%20Extended%20Multi-Doc%20QA.pdf)
  Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/alibaba-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20@aws%20Chen]%20MDCR_A%20Dataset%20for%20Multi-Document%20Conditional%20Reasoning.pdf)
  MDCR: A Dataset for Multi-Document Conditional Reasoning
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
  ![tags](https://img.shields.io/badge/aws-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20Buchmann]%20Attribute%20or%20Abstain_Large%20Language%20Models%20as%20Long%20Document%20Assistants.pdf)
  Attribute or Abstain: Large Language Models as Long Document Assistants
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Buchmann-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20Ma]%20Unifying%20Multimodal%20Retrieval%20via%20Document%20Screenshot%20Embedding.pdf)
  Unifying Multimodal Retrieval via Document Screenshot Embedding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Ma-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20Mass]%20More%20Bang%20for%20your%20Context_Virtual%20Documents%20for%20Question%20Answering%20over%20Long%20Documents.pdf)
  More Bang for your Context: Virtual Documents for Question Answering over Long Documents
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Mass-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20Phogat]%20Fine-tuning%20Smaller%20Language%20Models%20for%20Question%20Answering%20over%20Financial%20Documents.pdf)
  Fine-tuning Smaller Language Models for Question Answering over Financial Documents
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Phogat-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20Ye]%20GlobeSumm_A%20Challenging%20Benchmark%20Towards%20Unifying%20Multi-lingual,%20Cross-lingual%20and%20Multi-document%20News%20Summarization.pdf)
  GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Ye-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20Zhang]%20Modeling%20Layout%20Reading%20Order%20as%20Ordering%20Relations%20for%20Visually-rich%20Document%20Understanding.pdf)
  Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhang-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@emnlp%20Zhuang]%20PromptReps_Prompting%20Large%20Language%20Models%20to%20Generate%20Dense%20and%20Sparse%20Representations%20for%20Zero-Shot%20Document%20Retrieval.pdf)
  PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhuang-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@google%20Perot]%20LMDX_Language%20Model-based%20Document%20Information%20Extraction%20and%20Localization.pdf)
  LMDX: Language Model-based Document Information Extraction and Localization
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Perot-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@iclr%20Li]%20ChuLo_Chunk-Level%20Key%20Information%20Representation%20for%20Long%20Document%20Processing.pdf)
  ChuLo: Chunk-Level Key Information Representation for Long Document Processing
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@microsoft%20Wang]%20DLAFormer_An%20End-to-End%20Transformer%20For%20Document%20Layout%20Analysis.pdf)
  DLAFormer: An End-to-End Transformer For Document Layout Analysis
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@microsoft%20Xia]%20Vision%20Language%20Models%20for%20Spreadsheet%20Understanding_Challenges%20and%20Opportunities.pdf)
  Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Xia-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@mm%20Lin]%20PEneo_Unifying%20Line%20Extraction,%20Line%20Grouping,%20and%20Entity%20Linking%20for%20End-to-end%20Document%20Pair%20Extraction.pdf)
  PEneo: Unifying Line Extraction, Line Grouping, and Entity Linking for End-to-end Document Pair Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Lin-red?style=flat)
  ![tags](https://img.shields.io/badge/mm-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20@sail%20Zhang]%20Document%20Parsing%20Unveiled_Techniques,%20Challenges,%20and%20Prospects%20for%20Structured%20Information%20Extraction.pdf)
  Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhang-red?style=flat)
  ![tags](https://img.shields.io/badge/sail-blue?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Abdallah]%20CORU_Comprehensive%20Post-OCR%20Parsing%20and%20Receipt%20Understanding%20Dataset.pdf)
  CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Abdallah-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Abdallah]%20Transformers%20and%20Language%20Models%20in%20Form%20Understanding_A%20Comprehensive%20Review%20of%20Scanned%20Document%20Analysis.pdf)
  Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Abdallah-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Adhikari]%20A%20Comparative%20Study%20of%20PDF%20Parsing%20Tools%20Across%20Diverse%20Document%20Categories.pdf)
  A Comparative Study of PDF Parsing Tools Across Diverse Document Categories
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Adhikari-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Biswas]%20DocSynthv2_A%20Practical%20Autoregressive%20Modeling%20for%20Document%20Generation.pdf)
  DocSynthv2: A Practical Autoregressive Modeling for Document Generation
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Biswas-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Boytsov]%20Understanding%20Performance%20of%20Long-Document%20Ranking%20Models%20through%20Comprehensive%20Evaluation%20and%20Leaderboarding.pdf)
  Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Boytsov-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Bronnec]%20LOCOST_State-Space%20Models%20for%20Long%20Document%20Abstractive%20Summarization.pdf)
  LOCOST: State-Space Models for Long Document Abstractive Summarization
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Bronnec-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Cho]%20M3DocRAG_Multi-modal%20Retrieval%20is%20What%20You%20Need%20for%20Multi-page%20Multi-document%20Understanding.pdf)
  M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Cho-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Choi]%20Model-based%20Preference%20Optimization%20in%20Abstractive%20Summarization%20without%20Human%20Feedback.pdf)
  Model-based Preference Optimization in Abstractive Summarization without Human Feedback
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Choi-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Constantinou]%20Out-of-Distribution%20Detection%20with%20Attention%20Head%20Masking%20for%20Multimodal%20Document%20Classification.pdf)
  Out-of-Distribution Detection with Attention Head Masking for Multimodal Document Classification
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Constantinou-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Deng]%20LongDocURL_a%20Comprehensive%20Multimodal%20Long%20Document%20Benchmark%20Integrating%20Understanding,%20Reasoning,%20and%20Locating.pdf)
  LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Deng-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Ding]%20PDF-MVQA_A%20Dataset%20for%20Multimodal%20Information%20Retrieval%20in%20PDF-based%20Visual%20Question%20Answering.pdf)
  PDF-MVQA: A Dataset for Multimodal Information Retrieval in PDF-based Visual Question Answering
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Ding-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Futeral]%20mOSCAR_A%20Large-scale%20Multilingual%20and%20Multimodal%20Document-level%20Corpus.pdf)
  mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Futeral-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20He]%20DR-RAG_Applying%20Dynamic%20Document%20Relevance%20to%20RetrievalAugmented%20Generation%20for%20Question-Answering.pdf)
  DR-RAG: Applying Dynamic Document Relevance to RetrievalAugmented Generation for Question-Answering
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/He-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Hsu]%20M3T_A%20New%20Benchmark%20Dataset%20for%20Multi-Modal%20Document-Level%20Machine%20Translation.pdf)
  M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Hsu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Hu]%20mPLUG-DocOwl2_High-resolution%20Compressing%20for%20OCR-free%20Multi-page%20Document%20Understanding.pdf)
  mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Hu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Hui]%20UDA_A%20Benchmark%20Suite%20for%20Retrieval%20Augmented%20Generation%20in%20Real-world%20Document%20Analysis.pdf)
  UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Hui-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Li]%20Enhancing%20Visual%20Document%20Understanding%20with%20Contrastive%20Learning%20in%20Large%20Visual-Language%20Models.pdf)
  Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Liu]%20Fox_Focus%20Anywhere%20for%20Fine-grained%20Multi-page%20Document%20Understanding.pdf)
  Fox: Focus Anywhere for Fine-grained Multi-page Document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Liu]%20See%20then%20Tell_Enhancing%20Key%20Information%20Extraction%20with%20Vision%20Grounding.pdf)
  See then Tell: Enhancing Key Information Extraction with Vision Grounding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Liu]%20TextMonkey_An%20OCR-Free%20Large%20Multimodal%20Model%20for%20Understanding%20Document.pdf)
  TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Lu]%20A%20Bounding%20Box%20is%20Worth%20One%20Token_Interleaving%20Layout%20and%20Text%20in%20a%20Large%20Language%20Model%20for%20Document%20Understanding.pdf)
  A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Lu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Lu]%20From%20Text%20to%20Pixel_Advancing%20Long-Context%20Understanding%20in%20MLLMs.pdf)
  From Text to Pixel: Advancing Long-Context Understanding in MLLMs
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Lu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Mao]%20Visually%20Guided%20Generative%20Text-Layout%20Pre-training%20for%20Document%20Intelligence.pdf)
  Visually Guided Generative Text-Layout Pre-training for Document Intelligence
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Mao-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Mohammadshirazi]%20DocParseNet_Advanced%20Semantic%20Segmentation%20and%20OCR%20Embeddings%20for%20Efficient%20Scanned%20Document%20Annotation.pdf)
  DocParseNet: Advanced Semantic Segmentation and OCR Embeddings for Efficient Scanned Document Annotation
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Mohammadshirazi-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Qi]%20ADELIE_Aligning%20Large%20Language%20Models%20on%20Information%20Extraction.pdf)
  ADELIE: Aligning Large Language Models on Information Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Qi-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Shehzadi]%20A%20Hybrid%20Approach%20for%20Document%20Layout%20Analysis%20in%20Document%20images.pdf)
  A Hybrid Approach for Document Layout Analysis in Document images
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Shehzadi-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Xia]%20StructChart_Perception,%20Structuring,%20Reasoning%20for%20Visual%20Chart%20Understanding.pdf)
  StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Xia-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Xu]%20Large%20language%20models%20for%20generative%20information%20extraction_a%20survey.pdf)
  Large language models for generative information extraction: a survey
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Xu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Yu]%20TextHawk_Exploring%20Efficient%20Fine-Grained%20Perception%20of%20Multimodal%20Large%20Language%20Models.pdf)
  TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Yu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Zhang]%20SAIL_Sample-Centric%20In-Context%20Learning%20for%20Document%20Information%20Extraction.pdf)
  SAIL: Sample-Centric In-Context Learning for Document Information Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhang-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Zhao]%20TabPedia_Towards%20Comprehensive%20Visual%20Table%20Understanding%20with%20Concept%20Synergy.pdf)
  TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhao-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Zhu]%20MMDocBench_Benchmarking%20Large%20Vision-Language%20Models%20for%20Fine-Grained%20Visual%20Document%20Understanding.pdf)
  MMDocBench: Benchmarking Large Vision-Language Models for Fine-Grained Visual Document Understanding
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhu-red?style=flat)
- [ ] [:link:](./files/DOX/[2024%20Zmigrod]%20BuDDIE_A%20Business%20Document%20Dataset%20for%20Multi-task%20Information%20Extraction.pdf)
  BuDDIE: A Business Document Dataset for Multi-task Information Extraction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zmigrod-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@aaai%20Liskavets]%20Prompt%20Compression%20with%20Context-Aware%20Sentence%20Encoding%20for%20Fast%20and%20Improved%20LLM%20Inference.pdf)
  Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Liskavets-red?style=flat)
  ![tags](https://img.shields.io/badge/aaai-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@baidu%20Ni]%20PP-DocBee_Improving%20Multimodal%20Document%20Understanding%20Through%20a%20Bag%20of%20Tricks.pdf)
  PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Ni-red?style=flat)
  ![tags](https://img.shields.io/badge/baidu-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@ibm%20GraniteVision]%20Granite%20Vision_A%20Lightweight,%20Open-Source%20Multimodal%20Model%20for%20Enterprise%20Intelligence.pdf)
  Granite Vision: A Lightweight, Open-Source Multimodal Model for Enterprise Intelligence
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/GraniteVision-red?style=flat)
  ![tags](https://img.shields.io/badge/ibm-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@ibm%20Nassar]%20SmolDocling_An%20ultra-compact%20vision-language%20model%20for%20end-to-end%20multi-modal%20document%20conversion.pdf)
  SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Nassar-red?style=flat)
  ![tags](https://img.shields.io/badge/ibm-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@ibm%20Wasserman]%20REAL-MM-RAG_A%20Real-World%20Multi-Modal%20Retrieval%20Benchmark.pdf)
  REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Wasserman-red?style=flat)
  ![tags](https://img.shields.io/badge/ibm-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@iclr%20Chen]%20Graph-based%20Document%20Structure%20Analysis.pdf)
  Graph-based Document Structure Analysis
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@iclr%20Chen]%20LoRA-Contextualizing%20Adaptation%20of%20Large%20Multimodal%20Models%20for%20Multi-page%20Document%20Understanding.pdf)
  LoRA-Contextualizing Adaptation of Large Multimodal Models for Multi-page Document Understanding
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@iclr%20Faysse]%20ColPali_Efficient%20Document%20Retrieval%20with%20Vision%20Language%20Models.pdf)
  ColPali: Efficient Document Retrieval with Vision Language Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Faysse-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@iclr%20Huang]%20Mini-Monkey_Alleviating%20the%20Semantic%20Sawtooth%20Effect%20for%20Lightweight%20MLLMs%20via%20Complementary%20Image%20Pyramid.pdf)
  Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Huang-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@iclr%20Morris]%20Contextual%20Document%20Embeddings.pdf)
  Contextual Document Embeddings
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Morris-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@iclr%20Yu]%20VisRAG_Vision-based%20Retrieval-augmented%20Generation%20on%20Multi-modality%20Documents.pdf)
  VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Yu-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@iclr%20Zhu]%20Enhancing%20Document%20Understanding%20with%20Group%20Position%20Embedding_A%20Novel%20Approach%20to%20Incorporate%20Layout%20Information.pdf)
  Enhancing Document Understanding with Group Position Embedding: A Novel Approach to Incorporate Layout Information
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhu-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20@neurips%20Wang]%20CharXiv_Charting%20Gaps%20in%20Realistic%20Chart%20Understanding%20in%20Multimodal%20LLMs.pdf)
  CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/neurips-blue?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Barboule]%20Survey%20on%20Question%20Answering%20over%20Visually%20Rich%20Documents_Methods,%20Challenges,%20and%20Trends.pdf)
  Survey on Question Answering over Visually Rich Documents: Methods, Challenges, and Trends
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Barboule-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Gutteridge]%20Judge%20a%20Book%20by%20its%20Cover_Investigating%20Multi-Modal%20LLMs%20for%20Multi-Page%20Handwritten%20Document%20Transcription.pdf)
  Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page Handwritten Document Transcription
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Gutteridge-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Hu]%20DocMamba_Efficient%20Document%20Pre-training%20with%20State%20Space%20Model.pdf)
  DocMamba: Efficient Document Pre-training with State Space Model
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Hu-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Khang]%20KIEval_Evaluation%20Metric%20for%20Document%20Key%20Information%20Extraction.pdf)
  KIEval: Evaluation Metric for Document Key Information Extraction
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Khang-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Le]%20QID_Efficient%20Query-Informed%20ViTs%20in%20Data-Scarce%20Regimes%20for%20OCR-free%20Visual%20Document%20Understanding.pdf)
  QID: Efficient Query-Informed ViTs in Data-Scarce Regimes for OCR-free Visual Document Understanding
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Le-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Li]%20GDI-Bench_A%20Benchmark%20for%20General%20Document%20Intelligence%20with%20Vision%20and%20Reasoning%20Decoupling.pdf)
  GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Ouyang]%20OmniDocBench_Benchmarking%20Diverse%20PDF%20Document%20Parsing%20with%20Comprehensive%20Annotations.pdf)
  OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Ouyang-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Poznanski]%20olmOCR_Unlocking%20Trillions%20of%20Tokens%20in%20PDFs%20with%20Vision%20Language%20Models.pdf)
  olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Poznanski-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Suri]%20VisDoM_Multi-Document%20QA%20with%20Visually%20Rich%20Elements%20Using%20Multimodal%20Retrieval-Augmented%20Generation.pdf)
  VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Suri-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Xie]%20PDF-WuKong_A%20Large%20Multimodal%20Model%20for%20Efficient%20Long%20PDF%20Reading%20with%20End-to-End%20Sparse%20Sampling.pdf)
  PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Xie-red?style=flat)
- [ ] [:link:](./files/DOX/[2025%20Yu]%20OmniParser%20V2_Structured-Points-of-Thought%20for%20Unified%20Visual%20Text%20Parsing%20and%20Its%20Generality%20to%20Multimodal%20Large%20Language%20Models.pdf)
  OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Yu-red?style=flat)
<!-- AUTOGENERATED_DOX -->

## :parrot: NLP

<!-- AUTOGENERATED_NLP -->
- [ ] [:link:](./files/NLP/[2013%20Mikolov]%20Distributed%20Representations%20of%20Words%20and%20Phrases%20and%20their%20Compositionality.pdf)
  Distributed Representations of Words and Phrases and their Compositionality
  ![year](https://img.shields.io/badge/2013-green?style=flat)
  ![auth](https://img.shields.io/badge/Mikolov-red?style=flat)
- [ ] [:link:](./files/NLP/[2013%20Mikolov]%20Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space.pdf)
  Efficient Estimation of Word Representations in Vector Space
  ![year](https://img.shields.io/badge/2013-green?style=flat)
  ![auth](https://img.shields.io/badge/Mikolov-red?style=flat)
- [ ] [:link:](./files/NLP/[2017%20@google%20#milestone%20Vaswani]%20Attention%20Is%20All%20You%20Need.pdf)
  Attention Is All You Need
  ![year](https://img.shields.io/badge/2017-green?style=flat)
  ![auth](https://img.shields.io/badge/Vaswani-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2018%20@google%20#milestone%20Devlin]%20BERT_Pre-Training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.pdf)
  BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding
  ![year](https://img.shields.io/badge/2018-green?style=flat)
  ![auth](https://img.shields.io/badge/Devlin-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2018%20@openai%20#gpt%20#milestone%20Radford]%20Improving%20Language%20Understanding%20by%20Generative%20Pre-Training.pdf)
  Improving Language Understanding by Generative Pre-Training
  ![year](https://img.shields.io/badge/2018-green?style=flat)
  ![auth](https://img.shields.io/badge/Radford-red?style=flat)
  ![tags](https://img.shields.io/badge/openai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2019%20@acl%20Dai]%20Transformer-XL_Attentive%20Language%20Models%20Beyond%20a%20Fixed-Length%20Context.pdf)
  Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Dai-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/NLP/[2019%20@google%20#T5%20#milestone%20Raffel]%20Exploring%20the%20Limits%20of%20Transfer%20Learning%20with%20a%20Unified%20Text-to-Text%20Transformer.pdf)
  Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Raffel-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2019%20@iclr%20Wang]%20GLUE_A%20Multi-Task%20Benchmark%20and%20Analysis%20Platform%20for%20Natural%20Language%20Understanding.pdf)
  GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/NLP/[2019%20@meta%20Liu]%20RoBERTa_A%20Robustly%20Optimized%20BERT%20Pretraining%20Approach.pdf)
  RoBERTa: A Robustly Optimized BERT Pretraining Approach
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2019%20@neurips%20Wang]%20SuperGLUE_A%20Stickier%20Benchmark%20for%20General-Purpose%20Language%20Understanding.pdf)
  SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/neurips-blue?style=flat)
- [ ] [:link:](./files/NLP/[2019%20@neurips%20Yang]%20XLNet_Generalized%20Autoregressive%20Pretraining%20for%20Language%20Understanding.pdf)
  XLNet: Generalized Autoregressive Pretraining for Language Understanding
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Yang-red?style=flat)
  ![tags](https://img.shields.io/badge/neurips-blue?style=flat)
- [ ] [:link:](./files/NLP/[2019%20@openai%20#RLHF%20Ziegler]%20Fine-Tuning%20Language%20Models%20from%20Human%20Preferences.pdf)
  Fine-Tuning Language Models from Human Preferences
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Ziegler-red?style=flat)
  ![tags](https://img.shields.io/badge/openai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2019%20@openai%20#gpt2%20Radford]%20Language%20Models%20are%20Unsupervised%20Multitask%20Learners.pdf)
  Language Models are Unsupervised Multitask Learners
  ![year](https://img.shields.io/badge/2019-green?style=flat)
  ![auth](https://img.shields.io/badge/Radford-red?style=flat)
  ![tags](https://img.shields.io/badge/openai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2020%20@acl%20Gururangan]%20Don't%20Stop%20Pretraining_Adapt%20Language%20Models%20to%20Domains%20and%20Tasks.pdf)
  Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
  ![year](https://img.shields.io/badge/2020-green?style=flat)
  ![auth](https://img.shields.io/badge/Gururangan-red?style=flat)
  ![tags](https://img.shields.io/badge/acl-blue?style=flat)
- [ ] [:link:](./files/NLP/[2020%20@facebook%20Lewis]%20Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks.pdf)
  Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
  ![year](https://img.shields.io/badge/2020-green?style=flat)
  ![auth](https://img.shields.io/badge/Lewis-red?style=flat)
  ![tags](https://img.shields.io/badge/facebook-blue?style=flat)
- [ ] [:link:](./files/NLP/[2020%20@google%20Guu]%20REALM_Retrieval-Augmented%20Language%20Model%20Pre-Training.pdf)
  REALM: Retrieval-Augmented Language Model Pre-Training
  ![year](https://img.shields.io/badge/2020-green?style=flat)
  ![auth](https://img.shields.io/badge/Guu-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2020%20@iclr%20Khandelwal]%20Generalization%20through%20Memorization_Nearest%20Neighbor%20Language%20Models.pdf)
  Generalization through Memorization: Nearest Neighbor Language Models
  ![year](https://img.shields.io/badge/2020-green?style=flat)
  ![auth](https://img.shields.io/badge/Khandelwal-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/NLP/[2020%20@openai%20#gpt3%20Brown]%20Language%20Models%20are%20Few-Shot%20Learners.pdf)
  Language Models are Few-Shot Learners
  ![year](https://img.shields.io/badge/2020-green?style=flat)
  ![auth](https://img.shields.io/badge/Brown-red?style=flat)
  ![tags](https://img.shields.io/badge/openai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2020%20@openai%20#milestone%20Kaplan]%20Scaling%20Laws%20for%20Neural%20Language%20Models.pdf)
  Scaling Laws for Neural Language Models
  ![year](https://img.shields.io/badge/2020-green?style=flat)
  ![auth](https://img.shields.io/badge/Kaplan-red?style=flat)
  ![tags](https://img.shields.io/badge/openai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2021%20@microsoft%20#finetuning%20Hu]%20LoRA_Low-Rank%20Adaptation%20of%20Large%20Language%20Models.pdf)
  LoRA: Low-Rank Adaptation of Large Language Models
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Hu-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/NLP/[2021%20@microsoft%20Mao]%20Generation-Augmented%20Retrieval%20for%20Open-Domain%20Question%20Answering.pdf)
  Generation-Augmented Retrieval for Open-Domain Question Answering
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Mao-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/NLP/[2021%20@naacl%20Li]%20Document-Level%20Event%20Argument%20Extraction%20by%20Conditional%20Generation.pdf)
  Document-Level Event Argument Extraction by Conditional Generation
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
  ![tags](https://img.shields.io/badge/naacl-blue?style=flat)
- [ ] [:link:](./files/NLP/[2021%20@nvidia%20#megatron-lm%20#distributed%20Narayanan]%20Efficient%20Large-Scale%20Language%20Model%20Training%20on%20GPU%20Clusters%20Using%20Megatron-LM.pdf)
  Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Narayanan-red?style=flat)
  ![tags](https://img.shields.io/badge/nvidia-blue?style=flat)
- [ ] [:link:](./files/NLP/[2021%20Das]%20Case-Based%20Reasoning%20for%20Natural%20Language%20Queries%20over%20Knowledge%20Bases.pdf)
  Case-Based Reasoning for Natural Language Queries over Knowledge Bases
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Das-red?style=flat)
- [ ] [:link:](./files/NLP/[2021%20Huang]%20WhiteningBERT_An%20Easy%20Unsupervised%20Sentence%20Embedding%20Approach.pdf)
  WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Huang-red?style=flat)
- [ ] [:link:](./files/NLP/[2021%20Liu]%20Pre-train,%20Prompt,%20and%20Predict_A%20Systematic%20Survey%20of%20Prompting%20Methods%20in%20Natural%20Language%20Processing.pdf)
  Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
- [ ] [:link:](./files/NLP/[2022%20#finetuning%20Liu]%20P-Tuning%20v2_Prompt%20Tuning%20Can%20Be%20Comparable%20to%20Fine-tuning%20Universally%20Across%20Scales%20and%20Tasks.pdf)
  P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@anthropic%20Hernandez]%20Scaling%20Laws%20and%20Interpretability%20of%20Learning%20from%20Repeated%20Data.pdf)
  Scaling Laws and Interpretability of Learning from Repeated Data
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Hernandez-red?style=flat)
  ![tags](https://img.shields.io/badge/anthropic-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@deepmind%20Borgeaud]%20Improving%20language%20models%20by%20retrieving%20from%20trillions%20of%20tokens.pdf)
  Improving language models by retrieving from trillions of tokens
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Borgeaud-red?style=flat)
  ![tags](https://img.shields.io/badge/deepmind-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@emnlp%20Min]%20Rethinking%20the%20Role%20of%20Demonstrations_What%20Makes%20In-Context%20Learning%20Work.pdf)
  Rethinking the Role of Demonstrations: What Makes In-Context Learning Work
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Min-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@google%20#prompt%20Wei]%20Chain-of-Thought%20Prompting%20Elicits%20Reasoning%20in%20Large%20Language%20Models.pdf)
  Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Wei-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@google%20Chowdhery]%20PaLM_Scaling%20Language%20Modeling%20with%20Pathways.pdf)
  PaLM: Scaling Language Modeling with Pathways
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Chowdhery-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@hai%20#milestone%20Bommasani]%20On%20the%20Opportunities%20and%20Risks%20of%20Foundation%20Models.pdf)
  On the Opportunities and Risks of Foundation Models
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Bommasani-red?style=flat)
  ![tags](https://img.shields.io/badge/hai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@icml%20Borgeaud]%20Improving%20language%20models%20by%20retrieving%20from%20trillions%20of%20tokens.pdf)
  Improving language models by retrieving from trillions of tokens
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Borgeaud-red?style=flat)
  ![tags](https://img.shields.io/badge/icml-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@microsoft%20Wang]%20Training%20Data%20is%20More%20Valuable%20than%20You%20Think_A%20Simple%20and%20Effective%20Method%20by%20Retrieving%20from%20Training%20Data.pdf)
  Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20@openai%20#RLHF%20Ouyang]%20Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback.pdf)
  Training language models to follow instructions with human feedback
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Ouyang-red?style=flat)
  ![tags](https://img.shields.io/badge/openai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2022%20Izacard]%20Atlas_Few-shot%20Learning%20with%20Retrieval%20Augmented%20Language%20Models.pdf)
  Atlas: Few-shot Learning with Retrieval Augmented Language Models
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Izacard-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20#agent_talk%20Shinn]%20Reflexion_Language%20Agents%20with%20Verbal%20Reinforcement%20Learning.pdf)
  Reflexion: Language Agents with Verbal Reinforcement Learning
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Shinn-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@cmu%20Liu]%20Pre-train,%20Prompt,%20and%20Predict_A%20Systematic%20Survey%20of%20Prompting%20Methods%20in%20Natural%20Language%20Processing.pdf)
  Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
  ![tags](https://img.shields.io/badge/cmu-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@emnlp%20Dankers]%20Memorisation%20Cartography_Mapping%20out%20the%20Memorisation-Generalisation%20Continuum%20in%20Neural%20Machine%20Translation.pdf)
  Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Dankers-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@emnlp%20Jacovi]%20Stop%20Uploading%20Test%20Data%20in%20Plain%20Text_Practical%20Strategies%20for%20Mitigating%20Data%20Contamination%20by%20Evaluation%20Benchmarks.pdf)
  Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Jacovi-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@emnlp%20Liu]%20G-EVAL_NLG%20Evaluation%20using%20GPT-4%20with%20Better%20Human%20Alignment.pdf)
  G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@emnlp%20Wang]%20Label%20Words%20are%20Anchors_An%20Information%20Flow%20Perspective%20for%20Understanding%20In-Context%20Learning.pdf)
  Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@google%20#RLHF%20Lee]%20RLAIF_Scaling%20Reinforcement%20Learning%20from%20Human%20Feedback%20with%20AI%20Feedback.pdf)
  RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Lee-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@google%20Cai]%20Large%20Language%20Models%20as%20Tool%20Makers.pdf)
  Large Language Models as Tool Makers
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Cai-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@google%20Google]%20Gemini_A%20Family%20of%20Highly%20Capable%20Multimodal%20Models.pdf)
  Gemini: A Family of Highly Capable Multimodal Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Google-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@google%20Yang]%20Large%20Language%20Models%20as%20Optimizers.pdf)
  Large Language Models as Optimizers
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Yang-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@iclr%20Zhou]%20Large%20Language%20Models%20Are%20Human-Level%20Prompt%20Engineers.pdf)
  Large Language Models Are Human-Level Prompt Engineers
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhou-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@meta%20#lama%20Touvron]%20LLaMA_Open%20and%20Efficient%20Foundation%20Language%20Models.pdf)
  LLaMA: Open and Efficient Foundation Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Touvron-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@meta%20#llama2%20Touvron]%20LLaMA%202_Open%20Foundation%20and%20Fine-Tuned%20Chat%20Models.pdf)
  LLaMA 2: Open Foundation and Fine-Tuned Chat Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Touvron-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@meta%20Schick]%20Toolformer_Language%20Models%20Can%20Teach%20Themselves%20to%20Use%20Tools.pdf)
  Toolformer: Language Models Can Teach Themselves to Use Tools
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Schick-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@meta%20Xiong]%20Effective%20Long-Context%20Scaling%20of%20Foundation%20Models.pdf)
  Effective Long-Context Scaling of Foundation Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Xiong-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@microsoft%20Sun]%20Retentive%20Network_A%20Successor%20to%20Transformer%20for%20Large%20Language%20Models.pdf)
  Retentive Network: A Successor to Transformer for Large Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Sun-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@mistral%20Mistral]%20Mistral%207B.pdf)
  Mistral 7B
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Mistral-red?style=flat)
  ![tags](https://img.shields.io/badge/mistral-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@neurips%20@meta%20Tirumala]%20D4_Improving%20LLM%20Pretraining%20via%20Document%20De-Duplication%20and%20Diversification.pdf)
  D4: Improving LLM Pretraining via Document De-Duplication and Diversification
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Tirumala-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@neurips%20Wang]%20DORIS-MAE_Scientific%20Document%20Retrieval%20using%20Multi-level%20Aspect-based%20Queries.pdf)
  DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/neurips-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@neurips%20Yang]%20ResMem_Learn%20what%20you%20can%20and%20memorize%20the%20rest.pdf)
  ResMem: Learn what you can and memorize the rest
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Yang-red?style=flat)
  ![tags](https://img.shields.io/badge/neurips-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@openai%20#gpt4%20OpenAI]%20GPT-4%20Technical%20Report.pdf)
  GPT-4 Technical Report
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/OpenAI-red?style=flat)
  ![tags](https://img.shields.io/badge/openai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@stanford%20#dpo%20Rafailov]%20Direct%20Preference%20Optimization_Your%20Language%20Model%20is%20Secretly%20a%20Reward%20Model.pdf)
  Direct Preference Optimization: Your Language Model is Secretly a Reward Model
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Rafailov-red?style=flat)
  ![tags](https://img.shields.io/badge/stanford-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20@stanford%20Schaeffer]%20Are%20Emergent%20Abilities%20of%20Large%20Language%20Models%20a%20Mirage.pdf)
  Are Emergent Abilities of Large Language Models a Mirage
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Schaeffer-red?style=flat)
  ![tags](https://img.shields.io/badge/stanford-blue?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Bommasani]%20The%20Foundation%20Model%20Transparency%20Index.pdf)
  The Foundation Model Transparency Index
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Bommasani-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Chang]%20A%20Survey%20on%20Evaluation%20of%20Large%20Language%20Models.pdf)
  A Survey on Evaluation of Large Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Chang-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Chang]%20Learning%20to%20Generate%20Better%20Than%20Your%20LLM.pdf)
  Learning to Generate Better Than Your LLM
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Chang-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Chen]%20ChatGPT's%20One-year%20Anniversary_Are%20Open-Source%20Large%20Language%20Models%20Catching%20up.pdf)
  ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Chen]%20Understanding%20Retrieval%20Augmentation%20for%20Long-Form%20Question%20Answering.pdf)
  Understanding Retrieval Augmentation for Long-Form Question Answering
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Gao]%20Retrieval-Augmented%20Generation%20for%20Large%20Language%20Models_A%20Survey.pdf)
  Retrieval-Augmented Generation for Large Language Models: A Survey
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Gao-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Goyal]%20News%20Summarization%20and%20Evaluation%20in%20the%20Era%20of%20GPT-3.pdf)
  News Summarization and Evaluation in the Era of GPT-3
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Goyal-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20He]%20AnnoLLM_Making%20Large%20Language%20Models%20to%20Be%20Better%20Crowdsourced%20Annotators.pdf)
  AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/He-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20He]%20Simplifying%20Transformer%20Blocks.pdf)
  Simplifying Transformer Blocks
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/He-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Huang]%20OPERA_Alleviating%20Hallucination%20in%20Multi-Modal%20Large%20Language%20Models%20via%20Over-Trust%20Penalty%20and%20Retrospection-Allocation.pdf)
  OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Huang-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Jiang]%20Active%20Retrieval%20Augmented%20Generation.pdf)
  Active Retrieval Augmented Generation
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Jiang-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Kaddour]%20Challenges%20and%20Applications%20of%20Large%20Language%20Models.pdf)
  Challenges and Applications of Large Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Kaddour-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Kwon]%20Efficient%20Memory%20Management%20for%20Large%20Language%20Model%20Serving%20with%20PagedAttention.pdf)
  Efficient Memory Management for Large Language Model Serving with PagedAttention
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Kwon-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Li]%20CoAnnotating_Uncertainty-Guided%20Work%20Allocation%20between%20Human%20and%20Large%20Language%20Models%20for%20Data%20Annotation.pdf)
  CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Lilian]%20LLM%20Powered%20Autonomous%20Agents.pdf)
  LLM Powered Autonomous Agents
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Lilian-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Long]%20Adapt%20in%20Contexts_Retrieval-Augmented%20Domain%20Adaptation%20via%20In-Context%20Learning.pdf)
  Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Long-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Patil]%20Gorilla_Large%20Language%20Model%20Connected%20with%20Massive%20APIs.pdf)
  Gorilla: Large Language Model Connected with Massive APIs
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Patil-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Ram]%20In-Context%20Retrieval-Augmented%20Language%20Models.pdf)
  In-Context Retrieval-Augmented Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Ram-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Sainz]%20GoLLIE_Annotation%20Guidelines%20improve%20Zero-Shot%20Information-Extraction.pdf)
  GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Sainz-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Shaikh]%20On%20Second%20Thought,%20Let's%20Not%20Think%20Step%20by%20Step!%20Bias%20and%20Toxicity%20in%20Zero-Shot%20Reasoning.pdf)
  On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Shaikh-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Sheng]%20S-LoRA_Serving%20Thousands%20of%20Concurrent%20LoRA%20Adapters.pdf)
  S-LoRA: Serving Thousands of Concurrent LoRA Adapters
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Sheng-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Shi]%20In-Context%20Pretraining_Language%20Modeling%20Beyond%20Document%20Boundaries.pdf)
  In-Context Pretraining: Language Modeling Beyond Document Boundaries
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Shi-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Siriwardhana]%20Improving%20the%20Domain%20Adaptation%20of%20Retrieval%20Augmented%20Generation%20(RAG)%20Models%20for%20Open%20Domain%20Question%20Answering.pdf)
  Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Siriwardhana-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Xu]%20Retrieval%20Meets%20Long%20Context%20Large%20Language%20Models.pdf)
  Retrieval Meets Long Context Large Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Xu-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Yao]%20Tree%20of%20Thoughts_Deliberate%20Problem%20Solving%20with%20Large%20Language%20Models.pdf)
  Tree of Thoughts: Deliberate Problem Solving with Large Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Yao-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Yin]%20A%20Survey%20on%20Multimodal%20Large%20Language%20Models.pdf)
  A Survey on Multimodal Large Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Yin-red?style=flat)
- [ ] [:link:](./files/NLP/[2023%20Zhao]%20A%20Survey%20of%20Large%20Language%20Models.pdf)
  A Survey of Large Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhao-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#agent%20Guo]%20Large%20Language%20Model%20based%20Multi-Agents_A%20Survey%20of%20Progress%20and%20Challenges.pdf)
  Large Language Model based Multi-Agents: A Survey of Progress and Challenges
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Guo-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#agent_talk%20Fourney]%20Magentic-One_A%20Generalist%20Multi-Agent%20System%20for%20Solving%20Complex%20Tasks.pdf)
  Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Fourney-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#agent_talk%20Liu]%20Large%20Language%20Model-Based%20Agents%20for%20Software%20Engineering_A%20Survey.pdf)
  Large Language Model-Based Agents for Software Engineering: A Survey
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#agent_talk%20Raad]%20Scaling%20Instructable%20Agents%20Across%20Many%20Simulated%20Worlds.pdf)
  Scaling Instructable Agents Across Many Simulated Worlds
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Raad-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#agent_talk%20Wang]%20Chain-of-Thought%20Reasoning%20Without%20Prompting.pdf)
  Chain-of-Thought Reasoning Without Prompting
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#agent_talk%20Wu]%20ReFT_Representation%20Finetuning%20for%20Language%20Models.pdf)
  ReFT: Representation Finetuning for Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wu-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#agent_talk%20Zhuge]%20Agent-as-a-Judge_Evaluate%20Agents%20with%20Agents.pdf)
  Agent-as-a-Judge: Evaluate Agents with Agents
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhuge-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#finetuning%20Parthasarathy]%20The%20Ultimate%20Guide%20to%20Fine-Tuning%20LLMs%20from%20Basics%20to%20Breakthroughs_An%20Exhaustive%20Review%20of%20Technologies,%20Research,%20Best%20Practices,%20Applied%20Research%20Challenges%20and%20Opportunities.pdf)
  The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Parthasarathy-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#preference-learning%20Gao]%20Towards%20a%20Unified%20View%20of%20Preference%20Learning%20for%20Large%20Language%20Models_A%20Survey.pdf)
  Towards a Unified View of Preference Learning for Large Language Models: A Survey
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Gao-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20#reflection-tuning%20Li]%20Selective%20Reflection-Tuning_Student-Selected%20Data%20Recycling%20for%20LLM%20Instruction-Tuning.pdf)
  Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@Anthropic%20Anthropic]%20Building%20Effective%20Agents.pdf)
  Building Effective Agents
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Anthropic-red?style=flat)
  ![tags](https://img.shields.io/badge/Anthropic-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@aaai%20Chen]%20Benchmarking%20Large%20Language%20Models%20in%20Retrieval-Augmented%20Generation.pdf)
  Benchmarking Large Language Models in Retrieval-Augmented Generation
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
  ![tags](https://img.shields.io/badge/aaai-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@acl%20@alibaba]%20On%20the%20Role%20of%20Long-tail%20Knowledge%20in%20Retrieval%20Augmented%20Large%20Language%20Models.pdf)
  On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/@alibaba-red?style=flat)
  ![tags](https://img.shields.io/badge/alibaba]-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@alibaba%20Liu]%20CoFE-RAG_A%20Comprehensive%20Full-chain%20Evaluation%20Framework%20for%20Retrieval-Augmented%20Generation%20with%20Enhanced%20Data%20Diversity.pdf)
  CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
  ![tags](https://img.shields.io/badge/alibaba-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@apple%20McKinzie]%20MM1_Methods,%20Analysis%20&%20Insights%20from%20Multimodal%20LLM%20Pre-training.pdf)
  MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/McKinzie-red?style=flat)
  ![tags](https://img.shields.io/badge/apple-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@cohere%20Ustun]%20Aya%20Model_An%20Instruction%20Finetuned%20Open-Access%20Multilingual%20Language%20Model.pdf)
  Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Ustun-red?style=flat)
  ![tags](https://img.shields.io/badge/cohere-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@emnlp%20Wang]%20Searching%20for%20Best%20Practices%20in%20Retrieval-Augmented%20Generation.pdf)
  Searching for Best Practices in Retrieval-Augmented Generation
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@emnlp%20Zhao]%20LongAgent_Scaling%20Language%20Models%20to%20128k%20Context%20through%20Multi-Agent%20Collaboration.pdf)
  LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhao-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@google%20#gemini%20Google]%20Gemini%201.5_Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%20of%20context.pdf)
  Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Google-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@google%20Munkhdalai]%20Leave%20No%20Context%20Behind_Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention.pdf)
  Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Munkhdalai-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@google%20Petridis]%20ConstitutionalExperts_Training%20a%20Mixture%20of%20Principle-based%20Prompts.pdf)
  ConstitutionalExperts: Training a Mixture of Principle-based Prompts
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Petridis-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@ibm%20Asthana]%20Enterprise%20Benchmarks%20for%20Large%20Language%20Model%20Evaluation.pdf)
  Enterprise Benchmarks for Large Language Model Evaluation
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Asthana-red?style=flat)
  ![tags](https://img.shields.io/badge/ibm-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@iclr%20Jimenez]%20SWE-bench_Can%20Language%20Models%20Resolve%20Real-World%20GitHub%20Issues?.pdf)
  SWE-bench: Can Language Models Resolve Real-World GitHub Issues?
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Jimenez-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@iclr%20Sun]%20Enhancing%20Chain-of-Thoughts%20Prompting%20with%20Iterative%20Bootstrapping%20in%20Large%20Language%20Models.pdf)
  Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Sun-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@meta%20#reflection-tuning%20Ye]%20Physics%20of%20Language%20Models_Part%202.2,%20How%20to%20Learn%20From%20Mistakes%20on%20Grade-School%20Math%20Problems.pdf)
  Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Ye-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@meta%20Hao]%20Training%20Large%20Language%20Models%20to%20Reason%20in%20a%20Continuous%20Latent%20Space.pdf)
  Training Large Language Models to Reason in a Continuous Latent Space
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Hao-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@microsoft%20Qi]%20Mutual%20Reasoning%20Makes%20Smaller%20LLMs%20Stronger%20Problem-Solvers.pdf)
  Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Qi-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@microsoft%20Sun]%20You%20Only%20Cache%20Once_Decoder-Decoder%20Architectures%20for%20Language%20Models.pdf)
  You Only Cache Once: Decoder-Decoder Architectures for Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Sun-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@neurips%20@microsoft%20Lin]%20RHO-1_Not%20All%20Tokens%20Are%20What%20You%20Need.pdf)
  RHO-1: Not All Tokens Are What You Need
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Lin-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@neurips%20Tong]%20Cambrian-1_A%20Fully%20Open,%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs.pdf)
  Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Tong-red?style=flat)
  ![tags](https://img.shields.io/badge/neurips-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@nvidia%20Hsieh]%20RULER_What's%20the%20Real%20Context%20Size%20of%20Your%20Long-Context%20Language%20Models.pdf)
  RULER: What's the Real Context Size of Your Long-Context Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Hsieh-red?style=flat)
  ![tags](https://img.shields.io/badge/nvidia-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@nvidia%20Nvidia]%20Nemotron-4%20340B%20Technical%20Report.pdf)
  Nemotron-4 340B Technical Report
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Nvidia-red?style=flat)
  ![tags](https://img.shields.io/badge/nvidia-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@pku%20Liao]%20TPO_Aligning%20Large%20Language%20Models%20with%20Multi-branch%20&%20Multi-step%20Preference%20Trees.pdf)
  TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Liao-red?style=flat)
  ![tags](https://img.shields.io/badge/pku-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@stanford%20Hewitt]%20Instruction%20Following%20without%20Instruction%20Tuning.pdf)
  Instruction Following without Instruction Tuning
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Hewitt-red?style=flat)
  ![tags](https://img.shields.io/badge/stanford-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20@stanford%20Sun]%20Learning%20to%20(Learn%20at%20Test%20Time)_RNNs%20with%20Expressive%20Hidden%20States.pdf)
  Learning to (Learn at Test Time): RNNs with Expressive Hidden States
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Sun-red?style=flat)
  ![tags](https://img.shields.io/badge/stanford-blue?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Chen]%20Octopus%20v2_On-device%20language%20model%20for%20super%20agent.pdf)
  Octopus v2: On-device language model for super agent
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Doan]%20Vintern-1B_An%20Efficient%20Multimodal%20Large%20Language%20Model%20for%20Vietnamese.pdf)
  Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Doan-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Du]%20Understanding%20Emergent%20Abilities%20of%20Language%20Models%20from%20the%20Loss%20Perspective.pdf)
  Understanding Emergent Abilities of Language Models from the Loss Perspective
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Du-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Durante]%20Agent%20AI_Surveying%20the%20Horizons%20of%20Multimodal%20Interaction.pdf)
  Agent AI: Surveying the Horizons of Multimodal Interaction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Durante-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Edge]%20From%20Local%20to%20Global_A%20Graph%20RAG%20Approach%20to%20Query-Focused%20Summarization.pdf)
  From Local to Global: A Graph RAG Approach to Query-Focused Summarization
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Edge-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Fan]%20A%20Survey%20on%20RAG%20Meeting%20LLMs_Towards%20Retrieval-Augmented%20Large%20Language%20Models.pdf)
  A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Fan-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Gao]%20Higher%20Layers%20Need%20More%20LoRA%20Experts.pdf)
  Higher Layers Need More LoRA Experts
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Gao-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20He]%20Zero-Shot%20Cross-Lingual%20Document-Level%20Event%20Causality%20Identification%20with%20Heterogeneous%20Graph%20Contrastive%20Transfer%20Learning.pdf)
  Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/He-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Ji]%20Aligner_Achieving%20Efficient%20Alignment%20through%20Weak-to-Strong%20Correction.pdf)
  Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Ji-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Laurencon]%20Building%20and%20better%20understanding%20vision-language%20models_insights%20and%20future%20directions.pdf)
  Building and better understanding vision-language models: insights and future directions
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Laurencon-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Lesci]%20Causal%20Estimation%20of%20Memorisation%20Profiles.pdf)
  Causal Estimation of Memorisation Profiles
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Lesci-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Li]%20DataComp-LM_In%20search%20of%20the%20next%20generation%20of%20training%20sets%20for%20language%20models.pdf)
  DataComp-LM: In search of the next generation of training sets for language models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Ni]%20DIRAS_Efficient%20LLM%20Annotation%20of%20Document%20Relevance%20for%20Retrieval%20Augmented%20Generation.pdf)
  DIRAS: Efficient LLM Annotation of Document Relevance for Retrieval Augmented Generation
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Ni-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Ning]%20User-LLM_Efficient%20LLM%20Contextualization%20with%20User%20Embeddings.pdf)
  User-LLM: Efficient LLM Contextualization with User Embeddings
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Ning-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Peng]%20Graph%20Retrieval-Augmented%20Generation_A%20Survey.pdf)
  Graph Retrieval-Augmented Generation: A Survey
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Peng-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Sarthi]%20RAPTOR_Recursive%20Abstractive%20Processing%20for%20Tree-Organized%20Retrieval.pdf)
  RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Sarthi-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Sprague]%20To%20CoT%20or%20not%20to%20CoT?%20Chain-of-thought%20helps%20mainly%20on%20math%20and%20symbolic%20reasoning.pdf)
  To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Sprague-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Tam]%20Let%20Me%20Speak%20Freely.%20A%20Study%20on%20the%20Impact%20of%20Format%20Restrictions%20on%20Performance%20of%20Large%20Language%20Model.pdf)
  Let Me Speak Freely. A Study on the Impact of Format Restrictions on Performance of Large Language Model
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Tam-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Wallace]%20The%20Instruction%20Hierarchy_Training%20LLMs%20to%20Prioritize%20Privileged%20Instructions.pdf)
  The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wallace-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Wang]%20One%20Prompt%20is%20not%20Enough_Automated%20Construction%20of%20a%20Mixture-of-Expert%20Prompts.pdf)
  One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Wang]%20SeaEval%20for%20Multilingual%20Foundation%20Models_From%20Cross-Lingual%20Alignment%20to%20Cultural%20Reasoning.pdf)
  SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Zhang]%20In-Context%20Principle%20Learning%20from%20Mistakes.pdf)
  In-Context Principle Learning from Mistakes
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhang-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Zhang]%20RAFT_Adapting%20Language%20Model%20to%20Domain%20Specific%20RAG.pdf)
  RAFT: Adapting Language Model to Domain Specific RAG
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhang-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Zhang]%20Rethinking%20the%20Evaluation%20of%20Pre-trained%20Text-and-Layout%20Models%20from%20an%20Entity-Centric%20Perspective.pdf)
  Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an Entity-Centric Perspective
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhang-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20Zhao]%20A%20Survey%20of%20Large%20Language%20Models.pdf)
  A Survey of Large Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhao-red?style=flat)
- [ ] [:link:](./files/NLP/[2024%20lee]%20Quantifying%20Positional%20Biases%20in%20Text%20Embedding%20Models.pdf)
  Quantifying Positional Biases in Text Embedding Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/lee-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20#agent%20Luo]%20Large%20Language%20Model%20Agent_A%20Survey%20on%20Methodology,%20Applications%20and%20Challenges.pdf)
  Large Language Model Agent: A Survey on Methodology, Applications and Challenges
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Luo-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20#blog%20Silver]%20Welcome%20to%20the%20Era%20of%20Experience.pdf)
  Welcome to the Era of Experience
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Silver-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20#blog%20Weng]%20Why%20We%20Think.pdf)
  Why We Think
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Weng-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20@iclr%20Muennighoff]%20OLMoE_Open%20Mixture-of-Experts%20Language%20Models.pdf)
  OLMoE: Open Mixture-of-Experts Language Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Muennighoff-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/NLP/[2025%20@meta%20Golovneva]%20Multi-Token%20Attention.pdf)
  Multi-Token Attention
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Golovneva-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/NLP/[2025%20@salesforce%20#reasoning%20Ke]%20A%20Survey%20of%20Frontiers%20in%20LLM%20Reasoning_Inference%20Scaling,%20Learning%20to%20Reason,%20and%20Agentic%20Systems.pdf)
  A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Ke-red?style=flat)
  ![tags](https://img.shields.io/badge/salesforce-blue?style=flat)
- [ ] [:link:](./files/NLP/[2025%20Casper]%20The%20AI%20Agent%20Index.pdf)
  The AI Agent Index
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Casper-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20Cetin]%20An%20Evolved%20Universal%20Transformer%20Memory.pdf)
  An Evolved Universal Transformer Memory
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Cetin-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20DeepSeek]%20DeepSeek_R1.pdf)
  DeepSeek: R1
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/DeepSeek-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20Li]%20From%20System%201%20to%20System%202_A%20Survey%20of%20Reasoning%20Large%20Language%20Models.pdf)
  From System 1 to System 2: A Survey of Reasoning Large Language Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20Wu]%20CollabLLM_From%20Passive%20Responders%20to%20Active%20Collaborators.pdf)
  CollabLLM: From Passive Responders to Active Collaborators
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Wu-red?style=flat)
- [ ] [:link:](./files/NLP/[2025%20Zhao]%20Absolute%20Zero_Reinforced%20Self-play%20Reasoning%20with%20Zero%20Data.pdf)
  Absolute Zero: Reinforced Self-play Reasoning with Zero Data
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhao-red?style=flat)
<!-- AUTOGENERATED_NLP -->

## :sunrise: Multimodal

<!-- AUTOGENERATED_MM -->
- [ ] [:link:](./files/MM/[2021%20#stable-diff%20Rombach]%20High-Resolution%20Image%20Synthesis%20with%20Latent%20Diffusion%20Models.pdf)
  High-Resolution Image Synthesis with Latent Diffusion Models
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Rombach-red?style=flat)
- [ ] [:link:](./files/MM/[2021%20@google%20#vit%20Dosovitskiy]%20An%20Image%20is%20Worth%2016x16%20Words%20-%20Transformers%20for%20Image%20Recognition%20at%20Scale.pdf)
  An Image is Worth 16x16 Words - Transformers for Image Recognition at Scale
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Dosovitskiy-red?style=flat)
  ![tags](https://img.shields.io/badge/google-blue?style=flat)
- [ ] [:link:](./files/MM/[2021%20@microsoft%20#swin%20Liu]%20Swin%20Transformer%20-%20Hierarchical%20Vision%20Transformer%20using%20Shifted%20Windows.pdf)
  Swin Transformer - Hierarchical Vision Transformer using Shifted Windows
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/MM/[2021%20@microsoft%20#swin%20Liu]%20Swin%20Transformer%20V2%20-%20Scaling%20Up%20Capacity%20and%20Resolution.pdf)
  Swin Transformer V2 - Scaling Up Capacity and Resolution
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/MM/[2021%20@openai%20#clip%20Radford]%20Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision.pdf)
  Learning Transferable Visual Models From Natural Language Supervision
  ![year](https://img.shields.io/badge/2021-green?style=flat)
  ![auth](https://img.shields.io/badge/Radford-red?style=flat)
  ![tags](https://img.shields.io/badge/openai-blue?style=flat)
- [ ] [:link:](./files/MM/[2022%20#DiT%20#SoRA%20Peebles]%20Scalable%20Diffusion%20Models%20with%20Transformers.pdf)
  Scalable Diffusion Models with Transformers
  ![year](https://img.shields.io/badge/2022-green?style=flat)
  ![auth](https://img.shields.io/badge/Peebles-red?style=flat)
- [ ] [:link:](./files/MM/[2023%20@cvpr%20Li]%20Monkey_Image%20Resolution%20and%20Text%20Label%20Are%20Important%20Things%20for%20Large%20Multi-modal%20Models.pdf)
  Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
  ![tags](https://img.shields.io/badge/cvpr-blue?style=flat)
- [ ] [:link:](./files/MM/[2023%20@emnlp%20Zhao]%20Retrieving%20Multimodal%20Information%20for%20Augmented%20Generation_A%20Survey.pdf)
  Retrieving Multimodal Information for Augmented Generation: A Survey
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhao-red?style=flat)
  ![tags](https://img.shields.io/badge/emnlp-blue?style=flat)
- [ ] [:link:](./files/MM/[2023%20@neurips%20@microsoft%20Lin]%20LayoutPrompter_Awaken%20the%20Design%20Ability%20of%20Large%20Language%20Models.pdf)
  LayoutPrompter: Awaken the Design Ability of Large Language Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Lin-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/MM/[2023%20Bai]%20Qwen-VL_A%20Versatile%20Vision-Language%20Model%20for%20Understanding,%20Localization,%20Text%20Reading,%20and%20Beyond.pdf)
  Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Bai-red?style=flat)
- [ ] [:link:](./files/MM/[2023%20Bai]%20Sequential%20Modeling%20Enables%20Scalable%20Learning%20for%20Large%20Vision%20Models.pdf)
  Sequential Modeling Enables Scalable Learning for Large Vision Models
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Bai-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20#internvl-1.5%20Chen]%20How%20Far%20Are%20We%20to%20GPT-4V.%20Closing%20the%20Gap%20to%20Commercial%20Multimodal%20Models%20with%20Open-Source%20Suites.pdf)
  How Far Are We to GPT-4V. Closing the Gap to Commercial Multimodal Models with Open-Source Suites
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20#internvl-2.5%20Chen]%20Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%20Model,%20Data,%20and%20Test-Time%20Scaling.pdf)
  Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20#mini-internvl%20Gao]%20Mini-InternVL_A%20Flexible-Transfer%20Pocket%20Multimodal%20Model%20with%205%%20Parameters%20and%2090%%20Performance.pdf)
  Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Gao-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20@meta%20Bordes]%20An%20Introduction%20to%20Vision-Language%20Modeling.pdf)
  An Introduction to Vision-Language Modeling
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Bordes-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/MM/[2024%20@meta%20Zhou]%20Transfusion_Predict%20the%20Next%20Token%20and%20Diffuse%20Images%20with%20One%20Multi-Modal%20Model.pdf)
  Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhou-red?style=flat)
  ![tags](https://img.shields.io/badge/meta-blue?style=flat)
- [ ] [:link:](./files/MM/[2024%20@mmlab%20Zong]%20MoVA_Adapting%20Mixture%20of%20Vision%20Experts%20to%20Multimodal%20Context.pdf)
  MoVA: Adapting Mixture of Vision Experts to Multimodal Context
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Zong-red?style=flat)
  ![tags](https://img.shields.io/badge/mmlab-blue?style=flat)
- [ ] [:link:](./files/MM/[2024%20Chen]%20Next%20Token%20Prediction%20Towards%20Multimodal%20Intelligence_A%20Comprehensive%20Survey.pdf)
  Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20Cheng]%20YOLO-World_Real-Time%20Open-Vocabulary%20Object%20Detection.pdf)
  YOLO-World: Real-Time Open-Vocabulary Object Detection
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Cheng-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20Fuest]%20Diffusion%20Models%20and%20Representation%20Learning_A%20Survey.pdf)
  Diffusion Models and Representation Learning: A Survey
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Fuest-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20Gou]%20Eyes%20Closed,%20Safety%20On_Protecting%20Multimodal%20LLMs%20via%20Image-to-Text%20Transformation.pdf)
  Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Gou-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20Lin]%20MoE-LLaVA_Mixture%20of%20Experts%20for%20Large%20Vision-Language%20Models.pdf)
  MoE-LLaVA: Mixture of Experts for Large Vision-Language Models
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Lin-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20Masry]%20ChartInstruct_Instruction%20Tuning%20for%20Chart%20Comprehension%20and%20Reasoning.pdf)
  ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Masry-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20Wang]%20Qwen2-VL_Enhancing%20Vision-Language%20Model’s%20Perception%20of%20the%20World%20at%20Any%20Resolution.pdf)
  Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Wang-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20Yang]%20Law%20of%20Vision%20Representation%20in%20MLLMs.pdf)
  Law of Vision Representation in MLLMs
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Yang-red?style=flat)
- [ ] [:link:](./files/MM/[2024%20Yu]%20VisRAG_Vision-based%20Retrieval-augmented%20Generation%20on%20Multi-modality%20Documents.pdf)
  VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/Yu-red?style=flat)
- [ ] [:link:](./files/MM/[2025%20#internvl-3%20Zhu]%20InternVL3_Exploring%20Advanced%20Training%20and%20Test-Time%20Recipes%20for%20Open-Source%20Multimodal%20Models.pdf)
  InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Zhu-red?style=flat)
- [ ] [:link:](./files/MM/[2025%20#kimi%20KimiTeam]%20Kimi-VL%20Technical%20Report.pdf)
  Kimi-VL Technical Report
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/KimiTeam-red?style=flat)
- [ ] [:link:](./files/MM/[2025%20@aaai%20@huawei%20Rang]%20Eve_Efficient%20Multimodal%20Vision%20Language%20Models%20with%20Elastic%20Visual%20Experts.pdf)
  Eve: Efficient Multimodal Vision Language Models with Elastic Visual Experts
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Rang-red?style=flat)
  ![tags](https://img.shields.io/badge/huawei-blue?style=flat)
- [ ] [:link:](./files/MM/[2025%20@aaai%20Arif]%20HiRED_Attention-Guided%20Token%20Dropping%20for%20Efficient%20Inference%20of%20High-Resolution%20Vision-Language%20Models.pdf)
  HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Arif-red?style=flat)
  ![tags](https://img.shields.io/badge/aaai-blue?style=flat)
- [ ] [:link:](./files/MM/[2025%20@bytedance%20Guo]%20Seed1.5-VL%20Technical%20Report.pdf)
  Seed1.5-VL Technical Report
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Guo-red?style=flat)
  ![tags](https://img.shields.io/badge/bytedance-blue?style=flat)
- [ ] [:link:](./files/MM/[2025%20@iclr%20Neo]%20Towards%20Interpreting%20Visual%20Information%20Processing%20in%20Vision-Language%20Models.pdf)
  Towards Interpreting Visual Information Processing in Vision-Language Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Neo-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/MM/[2025%20@iclr%20Yang]%20SWE-bench%20Multimodal_Do%20AI%20Systems%20Generalize%20to%20Visual%20Software%20Domains?.pdf)
  SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Yang-red?style=flat)
  ![tags](https://img.shields.io/badge/iclr-blue?style=flat)
- [ ] [:link:](./files/MM/[2025%20@microsoft%20Li]%20Imagine%20while%20Reasoning%20in%20Space_Multimodal%20Visualization-of-Thought.pdf)
  Imagine while Reasoning in Space: Multimodal Visualization-of-Thought
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Li-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/MM/[2025%20@microsoft%20Yang]%20Magma_A%20Foundation%20Model%20for%20Multimodal%20AI%20Agents.pdf)
  Magma: A Foundation Model for Multimodal AI Agents
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Yang-red?style=flat)
  ![tags](https://img.shields.io/badge/microsoft-blue?style=flat)
- [ ] [:link:](./files/MM/[2025%20@salesforce%20Chen]%20BLIP3-o_A%20Family%20of%20Fully%20Open%20Unified%20Multimodal%20Models-Architecture,%20Training%20and%20Dataset.pdf)
  BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Chen-red?style=flat)
  ![tags](https://img.shields.io/badge/salesforce-blue?style=flat)
- [ ] [:link:](./files/MM/[2025%20Lu]%20InternVL-X_Advancing%20and%20Accelerating%20InternVL%20Series%20with%20Efficient%20Visual%20Token%20Compression.pdf)
  InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Lu-red?style=flat)
- [ ] [:link:](./files/MM/[2025%20QwenTeam]%20Qwen2.5-Omni%20Technical%20Report.pdf)
  Qwen2.5-Omni Technical Report
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/QwenTeam-red?style=flat)
- [ ] [:link:](./files/MM/[2025%20Qwen]%20Qwen2.5-VL%20Technical%20Report.pdf)
  Qwen2.5-VL Technical Report
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/QwenTeam-red?style=flat)
- [ ] [:link:](./files/MM/[2025%20Wolf]%20CM1%20-%20A%20Dataset%20for%20Evaluating%20Few-Shot%20Information%20Extraction%20with%20Large%20Vision%20Language%20Models.pdf)
  CM1 - A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Wolf-red?style=flat)
- [ ] [:link:](./files/MM/[2025%20Xu]%20Visual%20Planning_Let's%20Think%20Only%20with%20Images.pdf)
  Visual Planning: Let's Think Only with Images
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Xu-red?style=flat)
<!-- AUTOGENERATED_MM -->

## :inbox_tray: UNSORTED

<!-- AUTOGENERATED_UNSORTED -->
- [ ] [:link:](./files/UNSORTED/[2023%20Liu]%20iTransformer_Inverted%20Transformers%20Are%20Effective%20for%20Time%20Series%20Forecasting.pdf)
  iTransformer: Inverted Transformers Are Effective for Time Series Forecasting
  ![year](https://img.shields.io/badge/2023-green?style=flat)
  ![auth](https://img.shields.io/badge/Liu-red?style=flat)
- [ ] [:link:](./files/UNSORTED/[2024%20@stanford%20HAI]%20AI%20Index%20Report%202024.pdf)
  AI Index Report 2024
  ![year](https://img.shields.io/badge/2024-green?style=flat)
  ![auth](https://img.shields.io/badge/HAI-red?style=flat)
  ![tags](https://img.shields.io/badge/stanford-blue?style=flat)
- [ ] [:link:](./files/UNSORTED/[2025%20@sap%20Shen]%20ACL'25%20HELM.pdf)
  ACL'25 HELM
  ![year](https://img.shields.io/badge/2025-green?style=flat)
  ![auth](https://img.shields.io/badge/Shen-red?style=flat)
  ![tags](https://img.shields.io/badge/sap-blue?style=flat)
<!-- AUTOGENERATED_UNSORTED -->
